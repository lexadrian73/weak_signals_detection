{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d1b4d2-6ea5-4756-b7c8-6b900a60387d",
   "metadata": {},
   "source": [
    "## Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2301d07a-2763-4a05-9dc9-ade17b880cd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import unicodedata\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from unidecode import unidecode\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "# Plots renderizados en el navegador\n",
    "pio.renderers.default = 'browser'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639c6041-3e8c-43ee-b245-f12c92b31246",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cf94800-8daa-4569-b017-4ee978b8acdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "archivo_csv = os.path.join('data', 'ES', 'primicias', 'noticias_primicias_32180_2024-02-01.csv')\n",
    "\n",
    "dataset = pd.read_csv(archivo_csv)\n",
    "\n",
    "dataset['Fecha_Publicacion'] = dataset['Fecha_Publicacion'].str.extract(r'(\\d+ \\w+ \\d+ - \\d+:\\d+)', expand=False)\n",
    "\n",
    "meses_dic = {\n",
    "    'Ene': '01', 'Feb': '02', 'Mar': '03', 'Abr': '04', 'Apr': '04',\n",
    "    'May': '05', 'Jun': '06', 'Jul': '07', 'Ago': '08',\n",
    "    'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dic': '12'\n",
    "}\n",
    "\n",
    "# Reemplazar el mes por su número\n",
    "dataset['Fecha_Publicacion'] = dataset['Fecha_Publicacion'].str.replace(\n",
    "    r' (\\w+) ',\n",
    "    lambda x: ' ' + meses_dic.get(x.group(1), '00') + ' ',\n",
    "    regex=True\n",
    ")\n",
    "\n",
    "dataset['Fecha_Publicacion'] = pd.to_datetime(dataset['Fecha_Publicacion'], format='%d %m %Y - %H:%M', errors='coerce')\n",
    "\n",
    "# Eliminar filas con fechas inválidas\n",
    "dataset = dataset[dataset['Fecha_Publicacion'].notna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e36e59a-bb16-4c67-bc4b-a9aee2d85871",
   "metadata": {},
   "source": [
    "## Filtrado Fechas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc053cc-ad33-4c66-a4b5-e7e6c6e3785a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fecha_inicio = pd.to_datetime('2021-11-01')\n",
    "fecha_fin    = pd.to_datetime('2024-01-30')\n",
    "\n",
    "dataset = dataset[(dataset['Fecha_Publicacion'] >= fecha_inicio) & (dataset['Fecha_Publicacion'] <= fecha_fin)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d39821-ea16-43d3-a723-fb28134fbe3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fecha_minima = dataset['Fecha_Publicacion'].min()\n",
    "fecha_maxima = dataset['Fecha_Publicacion'].max()\n",
    "\n",
    "print(f\"Fecha mínima: {fecha_minima}\")\n",
    "print(f\"Fecha máxima: {fecha_maxima}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88737a8-1d9d-4c09-9ab0-42dcc0a1b9d3",
   "metadata": {},
   "source": [
    "## Drop data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5620f-3f90-4eeb-af8d-fd9326f2d488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(dataset.shape[0])\n",
    "dataset = dataset.dropna(subset=['Fecha_Publicacion'])\n",
    "print(dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eecdc0-ae88-40d3-8677-b57e4335fff4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/preprocessing.ipynb\n",
    "### https://huggingface.co/docs/transformers/main/en/notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7de246-7543-4760-9c69-0af001d90a35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cont_Titulo_Seccion = dataset['Titulo_Seccion'].value_counts()\n",
    "\n",
    "fig = px.bar(\n",
    "    x=cont_Titulo_Seccion.index,\n",
    "    y=cont_Titulo_Seccion.values,\n",
    "    labels={'x': 'Título de Sección', 'y': 'Cantidad de Noticias'},\n",
    "    title='Frecuencia de Noticias por Título de Sección'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991879c-f8d0-4825-aa62-0c3ce26da8e8",
   "metadata": {},
   "source": [
    "### Remove advertising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f4b444-e4ac-4564-9f62-8ab2fc0aa2f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_sponsored_content(texto):\n",
    "    try:\n",
    "        index = texto.index('Contenido Patrocinado')\n",
    "        return texto[:index]\n",
    "    except ValueError:\n",
    "        return texto\n",
    "\n",
    "dataset['Contenido_Parrafos'] = dataset['Contenido_Parrafos'].apply(remove_sponsored_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b75573",
   "metadata": {},
   "source": [
    "### Filtered by news category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c51138-e79c-46c7-b964-afaeea9b410a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type_news = ['Economía', 'Sucesos', 'Política', 'Sociedad', 'Lo último', 'Tecnociencia', 'Internacional',\n",
    "             'Seguridad', 'Elecciones presidenciales 2023', 'Seccionales 2023', 'Quito', 'Guayaquil', 'EN VIVO']\n",
    "type_news = ['Economía', 'Sucesos', 'Política', 'Sociedad', 'Internacional', 'Seguridad', 'Quito', 'Guayaquil']\n",
    "\n",
    "dataset = dataset[dataset['Titulo_Seccion'].isin(type_news)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b213a9d-e814-44b1-ba05-46a034e8804a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3237364-0c94-49a2-8075-92a0968d88b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31820351-a65f-46f6-b318-313039ca394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''El Gobierno anunció que invertirá este año más de USD 516 millones en obras de infraestructura eléctrica de subtransmisión, \n",
    "distribución y alumbrado público. 35USD y 15%'''\n",
    "\n",
    "text = unicodedata.normalize('NFD', text)\n",
    "text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "text = text.lower()\n",
    "\n",
    "\n",
    "cleaned_text_1 = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "print(cleaned_text_1)\n",
    "\n",
    "cleaned_text_2 = re.sub(r'[^\\w\\s]|[\\d_]+', '', text)\n",
    "print(cleaned_text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1804284-5f71-4dda-b25c-71dd23613c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "### Descargar las stopwords y configurar el tokenizador\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "### Cargar el modelo de spaCy para español\n",
    "# pip install spacy\n",
    "# python -m spacy download es_core_news_md\n",
    "\n",
    "# pip install stanza\n",
    "# stanza.download('es')\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "stop_words_unicode = {unidecode(word) for word in stop_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33046b92-b0d1-433e-8a73-e25753c51b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar modelos\n",
    "stanza_nlp = stanza.Pipeline('es')\n",
    "spacy_nlp  = spacy.load('es_core_news_md')\n",
    "\n",
    "def preprocess_and_clean(text):\n",
    "    if isinstance(text, str):\n",
    "        text = unicodedata.normalize('NFD', text)\n",
    "        text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "        return text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def lemmatize_with_stanza(text):\n",
    "    doc = stanza_nlp(text)\n",
    "    return ' '.join(\n",
    "        unidecode(word.lemma).lower()\n",
    "        for sentence in doc.sentences\n",
    "        for word in sentence.words\n",
    "        if unidecode(word.lemma).lower() not in stop_words_unicode\n",
    "    )\n",
    "\n",
    "def lemmatize_with_spacy(text):\n",
    "    doc = spacy_nlp(text)\n",
    "    lemmatized = ' '.join([\n",
    "        unidecode(token.lemma_)\n",
    "        for token in doc\n",
    "        if unidecode(token.lemma_) not in stop_words_unicode\n",
    "    ])\n",
    "    return lemmatized\n",
    "\n",
    "def tokenize_fw(text):\n",
    "    doc = spacy_nlp(text)\n",
    "    return [\n",
    "        token.text.lower() \n",
    "        for token in doc \n",
    "        if not token.is_space and not token.is_punct\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169fe197-4844-4612-8688-a15f2457de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas(desc=\"Preprocesando\")\n",
    "dataset['data_preprocess'] = dataset['Contenido_Parrafos'].progress_apply(preprocess_and_clean)\n",
    "\n",
    "tqdm.pandas(desc=\"Lematizando\")\n",
    "dataset['data_preprocess'] = dataset['data_preprocess'].progress_apply(lemmatize_with_spacy)\n",
    "# dataset['data_preprocess'] = dataset['data_preprocess'].progress_apply(lemmatize_with_stanza)\n",
    "\n",
    "tqdm.pandas(desc=\"Tokenizando\")\n",
    "dataset['data_tokenized'] = dataset['data_preprocess'].progress_apply(tokenize_fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3946f6-8590-4a16-bb14-37518a176d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dataset['data_preprocess'] = dataset['Contenido_Parrafos'].apply(preprocess_text)\n",
    "dataset['data_preprocess'] = dataset['data_preprocess'].apply(lemmatize_text)\n",
    "dataset['data_tokenized']  = dataset['data_preprocess'].apply(tokenize_fw)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1c021f-3926-4dd0-a8fe-07723964986d",
   "metadata": {},
   "source": [
    "### Example of row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439967c4-d3fc-4a75-ab94-ec67f98c0c46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a4c637-b2fc-4b82-b0ed-4d4a7c315dfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### OPEN Notebook 2_Exploratory_Data_Analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760d117f-c690-4bdf-baaa-fc6874697888",
   "metadata": {},
   "source": [
    "## Delete Head data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f1ae82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words_to_delete = [\n",
    "    'ser', 'haber', 'leer', 'tener', 'decir', 'ecuador', 'usd', 'pais', 'ano', \n",
    "    'suceso', 'segun', 'el', 'millon', '2023', 'gobierno', 'poder', 'nuevo',\n",
    "    '2022', 'sociedad', 'nacional', 'dos', 'hacer', 'persona', 'dia', '2021',\n",
    "    'enero', 'febrero', 'marzo', 'abril', 'mayo', 'junio', 'julio', 'agosto',\n",
    "    'septiembre', 'octubre', 'noviembre', 'diciembre']\n",
    "\n",
    "\n",
    "def remove_words(text, words):\n",
    "    pattern = r'\\b(?:' + '|'.join(map(re.escape, words)) + r')\\b'\n",
    "    return re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
    "\n",
    "def remove_from_tokens(tokens, delete_list):\n",
    "    return [word for word in tokens if word not in delete_list]\n",
    "\n",
    "dataset['data_preprocess'] = dataset['data_preprocess'].apply(lambda x: remove_words(x, words_to_delete))\n",
    "dataset['data_tokenized'] = dataset['data_tokenized'].apply(lambda tokens: remove_from_tokens(tokens, words_to_delete))\n",
    "###https://en.wikipedia.org/wiki/Long_tail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da7711",
   "metadata": {},
   "source": [
    "## Save datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e2f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_csv_fw  = os.path.join('data', 'processed', 'dataset_fw.csv')\n",
    "\n",
    "dataset_fw = dataset[['Fecha_Publicacion', 'data_tokenized']].copy()\n",
    "dataset_fw = dataset_fw.dropna()\n",
    "dataset_fw.to_csv(archivo_csv_fw, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12525009-0d94-45e2-baec-5c5ac8dbe9fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "archivo_csv_cl = os.path.join('data', 'processed', 'dataset_cl.csv')\n",
    "\n",
    "dataset_cl = dataset[['Titulo_Seccion', 'Fecha_Publicacion', 'Header_Tags', 'data_preprocess', 'data_tokenized']].copy()\n",
    "dataset_cl = dataset_cl.dropna()\n",
    "dataset_cl.to_csv(archivo_csv_cl, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63beb373-8f68-4343-9a36-9a555b34c005",
   "metadata": {},
   "source": [
    "## KIM & KEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742b9cda-d1d0-490c-9d47-cbcbc2e20553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir fechas a periodos diarios\n",
    "dataset_cl['Fecha_Publicacion'] = pd.to_datetime(dataset_cl['Fecha_Publicacion'], format='%d %b %Y')\n",
    "dataset_cl['time_segment'] = dataset_cl['Fecha_Publicacion'].dt.to_period('M')\n",
    "dataset_cl['time_segment'] = dataset_cl['time_segment'].astype('category')\n",
    "\n",
    "n = dataset_cl['time_segment'].nunique()\n",
    "\n",
    "term_frequencies     = []\n",
    "document_frequencies = []\n",
    "\n",
    "for period, group in dataset_cl.groupby('time_segment'):\n",
    "    all_words = [word for tokens in group['data_tokenized'] for word in tokens]\n",
    "    term_frequency = Counter(all_words)\n",
    "    document_frequency = Counter([word for tokens in group['data_tokenized'] for word in set(tokens)])\n",
    "    \n",
    "    for term in term_frequency:\n",
    "        term_frequencies.append({\n",
    "            'term': term,\n",
    "            'time_segment': period,\n",
    "            'term_frequency': term_frequency[term],\n",
    "            'total_terms': len(all_words)\n",
    "        })\n",
    "        document_frequencies.append({\n",
    "            'term': term,\n",
    "            'time_segment': period,\n",
    "            'document_frequency': document_frequency[term],\n",
    "            'total_documents': len(group)\n",
    "        })\n",
    "\n",
    "tf_df = pd.DataFrame(term_frequencies)\n",
    "df_df = pd.DataFrame(document_frequencies)\n",
    "\n",
    "# Fusionar frecuencias\n",
    "frequency_df = pd.merge(tf_df, df_df, on=['term', 'time_segment'])\n",
    "frequency_df['time_segment'] = frequency_df['time_segment'].astype('category')\n",
    "\n",
    "# Filtrar por umbral mínimo de frecuencia\n",
    "frequency_df = frequency_df[frequency_df['term_frequency'] >= 3]\n",
    "\n",
    "# Calcular DoV y DoD\n",
    "time_weight = 0.05\n",
    "frequency_df['DoV'] = (frequency_df['term_frequency'] / frequency_df['total_terms']) * (1 - time_weight * (n - frequency_df['time_segment'].cat.codes))\n",
    "frequency_df['DoD'] = (frequency_df['document_frequency'] / frequency_df['total_documents']) * (1 - time_weight * (n - frequency_df['time_segment'].cat.codes))\n",
    "\n",
    "# Agrupar por término\n",
    "result_df = frequency_df.groupby('term').agg({\n",
    "    'term_frequency': 'sum',\n",
    "    'document_frequency': 'sum',\n",
    "    'DoV': 'mean',\n",
    "    'DoD': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "result_df.columns = ['Keyword', 'Total Term Frequency', 'Total Document Frequency', 'DoV', 'DoD']\n",
    "result_df = result_df.sort_values(by='DoV', ascending=False)\n",
    "\n",
    "# Calcular medias\n",
    "mean_DoV = result_df['DoV'].mean()\n",
    "mean_DoD = result_df['DoD'].mean()\n",
    "\n",
    "# Crear mapas KEM y KIM\n",
    "fig_kem = px.scatter(result_df, x='Total Term Frequency', y='DoV',\n",
    "                     title='Keyword Emergence Map (KEM)',\n",
    "                     labels={'Total Term Frequency': 'Term Frequency', 'DoV': 'Degree of Visibility (DoV)'},\n",
    "                     hover_data={'Keyword': True},\n",
    "                     width=800, height=600)\n",
    "\n",
    "fig_kem.add_hline(y=mean_DoV, line_dash=\"dash\", line_color=\"red\")\n",
    "fig_kem.add_vline(x=result_df['Total Term Frequency'].mean(), line_dash=\"dash\", line_color=\"red\")\n",
    "fig_kem.update_traces(marker=dict(size=12, opacity=0.8))\n",
    "fig_kem.update_layout(font=dict(size=15))\n",
    "\n",
    "fig_kem.show()\n",
    "\n",
    "fig_kim = px.scatter(result_df, x='Total Document Frequency', y='DoD',\n",
    "                     title='Keyword Issue Map (KIM)',\n",
    "                     labels={'Total Document Frequency': 'Document Frequency', 'DoD': 'Degree of Diffusion (DoD)'},\n",
    "                     hover_data={'Keyword': True},\n",
    "                     width=800, height=600)\n",
    "\n",
    "fig_kim.add_hline(y=mean_DoD, line_dash=\"dash\", line_color=\"red\")\n",
    "fig_kim.add_vline(x=result_df['Total Document Frequency'].mean(), line_dash=\"dash\", line_color=\"red\")\n",
    "fig_kim.update_traces(marker=dict(size=12, opacity=0.8))\n",
    "fig_kim.update_layout(font=dict(size=15))\n",
    "\n",
    "fig_kim.show()\n",
    "\n",
    "# Identificar señales debiles\n",
    "result_df['quadrant_DoV'] = np.where((result_df['DoV'] > mean_DoV) & (result_df['Total Term Frequency'] < result_df['Total Term Frequency'].mean()), 'Weak', 'Other')\n",
    "result_df['quadrant_DoD'] = np.where((result_df['DoD'] > mean_DoD) & (result_df['Total Document Frequency'] < result_df['Total Document Frequency'].mean()), 'Weak', 'Other')\n",
    "\n",
    "# Encontrar keywords en KEM AND KIM\n",
    "weak_signals_f = result_df[(result_df['quadrant_DoV'] == 'Weak') & (result_df['quadrant_DoD'] == 'Weak')]\n",
    "\n",
    "output_file       = os.path.join('data', 'processed', 'weak_signals_kem_kim.csv')\n",
    "weak_signals_f.to_csv(output_file, index=False)\n",
    "\n",
    "print(weak_signals_f[['Keyword', 'DoV', 'DoD']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dff337-db25-4611-bf6d-a37f28c2a4a4",
   "metadata": {},
   "source": [
    "### Keywords maps ToyExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1c53db-471e-412c-bf25-4415d43779d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {\n",
    "#     'Fecha_Publicacion': [\n",
    "#         '2023-01-01 10:00', '2023-01-01 11:00', '2023-01-01 12:00',\n",
    "#         '2023-01-02 10:00', '2023-01-02 11:00', '2023-01-02 12:00',\n",
    "#         '2023-01-03 10:00', '2023-01-03 11:00', '2023-01-03 12:00',\n",
    "#         '2023-01-04 10:00', '2023-01-04 11:00', '2023-01-04 12:00',\n",
    "#         '2023-01-05 10:00', '2023-01-05 11:00', '2023-01-05 12:00',\n",
    "#         '2023-01-06 10:00', '2023-01-06 11:00', '2023-01-06 12:00',\n",
    "#         '2023-01-07 10:00', '2023-01-07 11:00', '2023-01-07 12:00'\n",
    "#     ],\n",
    "#     'Contenido_Parrafos': [\n",
    "#         \"Este es el primer documento. Habla sobre inteligencia artificial y aprendizaje profundo.\",\n",
    "#         \"Este es el segundo documento. Menciona redes neuronales y procesamiento de lenguaje natural.\",\n",
    "#         \"El tercer documento trata sobre visión por computadora y reconocimiento de imágenes.\",\n",
    "#         \"Este cuarto documento menciona big data y análisis de datos.\",\n",
    "#         \"El quinto documento habla sobre aprendizaje automático y algoritmos de machine learning.\",\n",
    "#         \"Este es el sexto documento y trata sobre minería de datos y extracción de conocimiento.\",\n",
    "#         \"El séptimo documento menciona robótica y automatización.\",\n",
    "#         \"Este octavo documento habla sobre sistemas embebidos y IoT.\",\n",
    "#         \"El noveno documento menciona ciberseguridad y protección de datos.\",\n",
    "#         \"El décimo documento trata sobre blockchain y criptomonedas.\",\n",
    "#         \"Este undécimo documento habla sobre realidad virtual y aumentada.\",\n",
    "#         \"El duodécimo documento menciona biotecnología y genómica.\",\n",
    "#         \"El decimotercer documento trata sobre energías renovables y sostenibilidad.\",\n",
    "#         \"Este decimocuarto documento menciona vehículos autónomos y conducción automática.\",\n",
    "#         \"El decimoquinto documento habla sobre inteligencia artificial y ética.\",\n",
    "#         \"Este decimosexto documento trata sobre cloud computing y servicios en la nube.\",\n",
    "#         \"El decimoséptimo documento menciona edge computing y procesamiento en el borde.\",\n",
    "#         \"Este decimoctavo documento habla sobre quantum computing y computación cuántica.\",\n",
    "#         \"El decimonoveno documento menciona software libre y código abierto.\",\n",
    "#         \"El vigésimo documento trata sobre economía digital y comercio electrónico.\",\n",
    "#         \"Este vigesimoprimer documento menciona marketing digital y publicidad en línea.\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# datasett = pd.DataFrame(data)\n",
    "# datasett['Fecha_Publicacion'] = pd.to_datetime(datasett['Fecha_Publicacion'])\n",
    "\n",
    "\n",
    "# datasett['data_preprocess'] = datasett['Contenido_Parrafos'].apply(preprocess_and_clean)\n",
    "# datasett['data_tokenized']  = datasett['data_preprocess'].apply(tokenize_fw)\n",
    "\n",
    "# # Calcular frecuencias\n",
    "# datasett['time_segment'] = datasett['Fecha_Publicacion'].dt.to_period('D')\n",
    "# datasett['time_segment'] = datasett['time_segment'].astype('category')\n",
    "# n = dataset['time_segment'].nunique()\n",
    "\n",
    "# term_frequencies     = []\n",
    "# document_frequencies = []\n",
    "\n",
    "# for period, group in datasett.groupby('time_segment'):\n",
    "#     all_words = [word for tokens in group['data_tokenized'] for word in tokens]\n",
    "#     term_frequency = Counter(all_words)\n",
    "#     document_frequency = Counter([word for tokens in group['data_tokenized'] for word in set(tokens)])\n",
    "    \n",
    "#     for term in term_frequency:\n",
    "#         term_frequencies.append({'term': term, 'time_segment': period, 'term_frequency': term_frequency[term], 'total_terms': len(all_words)})\n",
    "#         document_frequencies.append({'term': term, 'time_segment': period, 'document_frequency': document_frequency[term], 'total_documents': len(group)})\n",
    "\n",
    "# tf_df = pd.DataFrame(term_frequencies)\n",
    "# df_df = pd.DataFrame(document_frequencies)\n",
    "\n",
    "# # Fusionar frecuencias\n",
    "# frequency_df = pd.merge(tf_df, df_df, on=['term', 'time_segment'])\n",
    "# frequency_df['time_segment'] = frequency_df['time_segment'].astype('category')\n",
    "\n",
    "# # Calcular DoV y DoD\n",
    "# time_weight = 0.05\n",
    "# frequency_df['DoV'] = (frequency_df['term_frequency'] / frequency_df['total_terms']) * (1 - time_weight * (n - frequency_df['time_segment'].cat.codes))\n",
    "# frequency_df['DoD'] = (frequency_df['document_frequency'] / frequency_df['total_documents']) * (1 - time_weight * (n - frequency_df['time_segment'].cat.codes))\n",
    "\n",
    "# # Agrupar por término\n",
    "# result_df = frequency_df.groupby('term').agg({\n",
    "#     'term_frequency': 'sum',\n",
    "#     'document_frequency': 'sum',\n",
    "#     'DoV': 'mean',\n",
    "#     'DoD': 'mean'\n",
    "# }).reset_index()\n",
    "\n",
    "# result_df.columns = ['Keyword', 'Total Term Frequency', 'Total Document Frequency', 'DoV', 'DoD']\n",
    "# result_df = result_df.sort_values(by='DoV', ascending=False)\n",
    "\n",
    "# # Calcular medias\n",
    "# mean_DoV = result_df['DoV'].mean()\n",
    "# mean_DoD = result_df['DoD'].mean()\n",
    "\n",
    "# # Crear mapas KEM y KIM\n",
    "# fig_kem = px.scatter(result_df, x='Total Term Frequency', y='DoV',\n",
    "#                      title='Keyword Emergence Map (KEM)',\n",
    "#                      labels={'Total Term Frequency': 'Term Frequency', 'DoV': 'Degree of Visibility (DoV)'},\n",
    "#                      hover_data={'Keyword': True},\n",
    "#                      width=800, height=600)\n",
    "\n",
    "# fig_kem.add_hline(y=mean_DoV, line_dash=\"dash\", line_color=\"red\")\n",
    "# fig_kem.add_vline(x=result_df['Total Term Frequency'].mean(), line_dash=\"dash\", line_color=\"red\")\n",
    "# fig_kem.update_traces(marker=dict(size=12, opacity=0.8))\n",
    "# fig_kem.update_layout(font=dict(size=15))\n",
    "\n",
    "# fig_kem.show()\n",
    "\n",
    "# fig_kim = px.scatter(result_df, x='Total Document Frequency', y='DoD',\n",
    "#                      title='Keyword Issue Map (KIM)',\n",
    "#                      labels={'Total Document Frequency': 'Document Frequency', 'DoD': 'Degree of Diffusion (DoD)'},\n",
    "#                      hover_data={'Keyword': True},\n",
    "#                      width=800, height=600)\n",
    "\n",
    "# fig_kim.add_hline(y=mean_DoD, line_dash=\"dash\", line_color=\"red\")\n",
    "# fig_kim.add_vline(x=result_df['Total Document Frequency'].mean(), line_dash=\"dash\", line_color=\"red\")\n",
    "# fig_kim.update_traces(marker=dict(size=12, opacity=0.8))\n",
    "# fig_kim.update_layout(font=dict(size=15))\n",
    "\n",
    "# fig_kim.show()\n",
    "\n",
    "# # Identificar señales debiles\n",
    "# result_df['quadrant_DoV'] = np.where((result_df['DoV'] > mean_DoV) & (result_df['Total Term Frequency'] < result_df['Total Term Frequency'].mean()), 'Weak', 'Other')\n",
    "# result_df['quadrant_DoD'] = np.where((result_df['DoD'] > mean_DoD) & (result_df['Total Document Frequency'] < result_df['Total Document Frequency'].mean()), 'Weak', 'Other')\n",
    "\n",
    "# # Encontrar keywords en KEM & KIM\n",
    "# weak_signals_t = result_df[(result_df['quadrant_DoV'] == 'Weak') & (result_df['quadrant_DoD'] == 'Weak')]\n",
    "# print(weak_signals_t.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
